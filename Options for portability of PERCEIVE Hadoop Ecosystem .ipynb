{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "To address options for portability of PERCEIVE Hadoop Ecosystem and apply big data technology for PERCEIVE project, this notebook aims to build an Apache Nifi + Kite sdk + Apache Spark pipeline, and show the potential value and usage of this data pipeline.\n",
    "\n",
    "This article is consisted of three parts. For the first part, we will setup the environment and relevant components, and build dependency between each component. The second part will show a simple Nifi data flow from local and the web to HDFS that demonstrates several fundamental capabilities of Nifi.\n",
    "\n",
    "The third part will address the advantages of Nifi and show why the pipeline can potentially contribute to PERCEIVE project in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Before moving to next part, here is a brief introduction about Apache Nifi and Kite sdk.\n",
    "\n",
    "#### a. Apache NiFi:\n",
    "\n",
    "Nifi is an easy to use, powerful, and reliable dataflow tool that enables the automation of data flow between systems. Some of the use cases include, but are not limited to:\n",
    "\n",
    "- **Big Data Ingest**– Offers a simple, reliable and secure way to collect data streams.\n",
    "- **IoAT Optimization**– Allows organizations to overcome real world constraints such as limited or expensive bandwidth while ensuring data quality and reliability.\n",
    "- **Compliance**– Enables organizations to understand everything that happens to data in motion from its creation to its final resting place, which is particularly important for regulated industries that must retain and report on chain of custody.\n",
    "- **Digital Security**– Helps organizations collect large volumes of data from many sources and prioritize which data is brought back for analysis first, a critical capability given the time sensitivity of identifying security breaches\n",
    "\n",
    "Details of Nifi will be discussed later through the example of using Nifi and Kite.\n",
    "\n",
    "![egauge_portfolio](img/nifi_intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.  Kite sdk:\n",
    "\n",
    "The Kite SDK is a high-level data API that makes it easier to put data into Hadoop and to work with data once it's loaded. With Kite, it will be much convenient to maintain datasets and relevant metadata through Avro schema. Currently, Kite can work with file formats, including CSV, JSON, Avro, and Parquet. With Kite sdk, you can define how your data is stored, including Hive, HDFS, local file system, HBase, Amazon S3, and compress data: Snappy (default), Deflate, Bzip2, and Lzo.\n",
    "\n",
    "For fully leverage Kite sdk, the best way is to use Kite Dataset command line interface (CLI),which provides utility commands to perform essential tasks such as creating a schema and datset, importing data from a CSV file, and viewing the results.\n",
    "\n",
    "\n",
    "The following use case example may help to understand how Kite works:\n",
    "\n",
    "        Kite will handle how the data is stored. For example, if I wanted to store incoming CSV data into a Parquet formatted Hive table, I could use the Kite API to create a schema for my CSV data and then call the Kite API to create the Hive table for me. Kite also works with partitioned data and will automatically partition records when writing.[1]\n",
    "        \n",
    "Example of schema:\n",
    "```\n",
    "{\n",
    "  \"type\":\"record\",\n",
    "  \"name\":\"Movie\",\n",
    "  \"namespace\":\"org.kitesdk.examples.data\",\n",
    "  \"fields\":[\n",
    "    {\"name\":\"id\",\"type\":\"int\"},\n",
    "    {\"name\":\"title\",\"type\":\"string\"},\n",
    "    {\"name\":\"release_date\",\"type\":\"string\"},\n",
    "    {\"name\":\"imdb_url\",\"type\":\"string\"}\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Installation\n",
    "This tutorial was tested using the following environment and components:\n",
    "\n",
    "(Since the installation for the above components can easily be found through google, we will simply put the link here)\n",
    "\n",
    "- Ubuntu(64 bit) on VirualBox:\n",
    "    - https://linus.nci.nih.gov/bdge/installUbuntu.html\n",
    "- Apache Nifi 1.3.0:\n",
    "    - For installation: https://www.youtube.com/watch?v=ZTXRm7tqQs4\n",
    "    - User guide: https://nifi.apache.org/docs.html\n",
    "    - Required jdk environment\n",
    "- Apache Hadoop 2.8.1 + Apache Spark 2.2.0 + Scala 2.12.3:\n",
    "    - Need to change the version: https://medium.com/@ivanermilov/setting-up-hadoop-spark-hive-development-environment-on-ubuntu-94f0f8166ef1\n",
    "- Apache Kite sdk:\n",
    "    - http://kitesdk.org/docs/1.1.0/Install-Kite.html\n",
    "    - http://kitesdk.org/docs/1.0.0/Kite-SDK-Guide.html\n",
    "- Apache Hive 2.3.0 (optional):\n",
    "    - https://www.edureka.co/blog/apache-hive-installation-on-ubuntu\n",
    "\n",
    "If following the above tutorials to install these environment and components, please pay attention for the version used in these tutorials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Building the dependency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### a. Dependency between Nifi and Spark\n",
    "\n",
    "\"Apache Spark has become wildly popular for processing large quantities of data. One of the key features that Spark provides is the ability to process data in either a batch processing mode or a streaming mode with very little change to your code\".[2] \n",
    "\n",
    "However, in many context, operating on the data as soon as it is available can provides great benefits. Therefore, with the dependency between Apache Nifi and Apache Spark, the Spark application can directly perform streaming analysis and process data from Nifi.\n",
    "\n",
    "In [2] article, it introduces how to incorporate the Apache Nifi Receiver with the Spark application in Java. If your Spark application is built on Java and maintained by Maven, it is pretty easy to follow this article and add the receiver to the application's POM (Project Object Model).\n",
    "\n",
    "However, since most analysis jobs in PERCEIVE are using Python language, it is still a problem to built dependency through Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### b. Working with Kite sdk within Apache Nifi\n",
    "\n",
    "Although Kite is designed to work within Kite CLI, Nifi provides the following kite processors that realize part of Kite sdk functions, making most jobs out of Kite CLI: \n",
    "\n",
    "|Type|Version|Tags|Description|\n",
    "|--|-----|-----|-------------|\n",
    "|ConvertAvroSchema |1.3.0|convert,kite,avro|Converts recrods from one Avro schema to another|\n",
    "|ConvertCSVToAvro|1.3.0|csv,kite,avro|Converts CSV files to Avro according to an Avro Schema|\n",
    "|ConvertJSONToAvro|1.3.0|json,kite,avro|Converts JSON files to Avro according to an Avro Schema|\n",
    "|InferAvroSchema|1.3.0|schema, infer,csv,json,kite,avro|Examines the contents of the incoming FlowFile to infer an Avro schema|\n",
    "|StoreInKiteDataset|1.3.0|hive,hdfs,hadoop,kite,hbase|Stores Avro records in a Kite dataset|\n",
    "\n",
    "However, some critical jobs have to be done manually within Kite CLI, such as manually creating an Avro schema, updating a new Avro schema for current files, creating a hive table with parquet format, creating a partition strategy for datasets, and creating a directory in HDFS to store the Parquet data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Examples of Apache NiFi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Through a few examples, this section will demonstrate several fundamental capabilities of Nifi, how to work with NiFi and Kite sdk together, and how to configure several core processors of NiFi. These examples will accomplish the following tasks:\n",
    "\n",
    "- Load files from local and web URL\n",
    "- Filter files based on file format\n",
    "- Add custom attributes using NiFi Expression Language\n",
    "- Split files based on month\n",
    "- Store files into different folders within HDFS based on file attribute\n",
    "- Work with Kite sdk processors within NiFi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load files from local to HDFS\n",
    "\n",
    "The NiFi flow is shown below. At a very high-level, this dataflow will generate all files from local system and store into different month folders within HDFS. To realize dynamically configure the output path, we use NiFi Expression Language to add a custom attribute for each file based on file name. We will discuss the details and how to configure for each processor below.\n",
    "\n",
    "![egauge_portfolio](img/nifi_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 1. GetFile processor\n",
    "\n",
    "In my case, this processor reads all files under my input directory. Important configurations for this processor are:\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|Input Directory|The input directory from which to pull files |/home/yueliu/Documents/Full Disclosure/2002|\n",
    "|Keep source file|Whether to keep the original file after generating|false|\n",
    "|File Filter|which can use regular expression to filter files|[^\\\\.].\\* |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 2. RouteOnAttribute processor\n",
    "\n",
    "In NiFi, each FlowFile has a minimum set of attribute: filename, path, uuid, entryDate, lineageStartDate, and fileSize. The detail explanation of these common attribute can be found here [4]. RouteOnAttribute processor will route FlowFile based on the attributes that it contains. In this case, this processor will route FlowFile to the success relationship if the file name is end with \"csv\", while route FlowFile to the unmatched relationship if the condition is not qualified.\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|Routing Strategy|Specifies how to determine which relationship to use when evaluating the Expression Language |Route To Property name|\n",
    "|fileFormat|The property I added that is used to evaluate file attribute |${filename:endsWith('csv')}|\n",
    "\n",
    "User can add any number of property to generate appropriate FlowFiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 3. UpdateAttribute processor\n",
    "\n",
    "UpdateAttribute processor is used to add or update any number of user-defined attributes to a FlowFile. It is useful for adding statically configured values, as well as deriving attribute values dynamically by using the Expression Language. \n",
    "\n",
    "In this example, we use two UpdateAttribute processors to add a custom attribute to represent the month information for txt and csv FlowFile based on their file names. To better understand the NiFi Expression Language used below, one txt file name is '2002_Aug_1.txt', while one csv file name is 'Full_Disclosure_Mainling_List_Aug2002.csv'. More details about NiFi Expression Language can be found here [5].\n",
    "\n",
    "Add_Month_Attribute_For_CSV processor:\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|month|we add this attribute for each FlowFile based on NiFi Expression Language|${filename:getDelimitedField(5,'_'):substring(0,3)}|\n",
    "\n",
    "Configuration of the connection between RouteOnAttribute and Add_Month_Attribute_For_CSV: fileFormat relationship should be checked\n",
    "\n",
    "\n",
    "Add_Month_Attribute_For_TXT processor:\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|month|we add this attribute for each FlowFile based on NiFi Expression Language|${filename:getDelimitedField(2,'_')}|\n",
    "\n",
    "Configuration of the connection between RouteOnAttribute and Add_Month_Attribute_For_TXT: unmatched relationship should be checked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 4. PutHDFS processor:\n",
    "\n",
    "This processor will write FlowFile to Hadoop Distributed File System (hdfs). Given the custom attribute \"month\" we have created before for each FlowFile, we can use this attribute as a dynamic path for each FlowFile, thus organizing all FlowFiles within month folders in HDFS. \n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|Hadoop Configuration Resources|A file or comma separated list of files which contains the Hadoop file system configuration. We usually put the path of core-site.xml file here|/home/yueliu/Documents/hadoop-2.8.1/etc/hadoop/core-site.xml|\n",
    "|Directory|The parent HDFS directory to which files should be written. The directory will be created if it does not exist.|/sample/${month}|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Check within HDFS\n",
    "\n",
    "Use the following code in terminal to check the result of the above NiFi dataflow:\n",
    "```\n",
    "hadoop fs -ls /sample/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load xml file from URL to HDFS\n",
    "\n",
    "The NiFi flow is shown below. At a very high-level, this dataflow will a XML file from a URL and store into HDFS. In this example, we add a custom attribute to represent the file format to help you understand how attribute works in NiFi. We will discuss the details and how to configure for each processor below.\n",
    "\n",
    "![egauge_portfolio](img/nifi_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. GetHTTP processor\n",
    "\n",
    "GetHTTP processor can fetches data from an HTTP URL and write the data to the content of a FlowFile. In this example, we pull a XML file from the link below. \n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|URL|The URL to pull from |http://capec.mitre.org/data/xml/capec_v2.11.xml|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. UpdateAttribute processor\n",
    "\n",
    "In this example, we use the Expression Language to retrieve format information from FlowFile name and assign to a custom attribute named \"fileFormat\". \n",
    "\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|fileFormat|we add this attribute for each FlowFile based on NiFi Expression Language|${filename:substringAfterLast('.')}|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. PutHDFS processor:\n",
    "\n",
    "This processor will write FlowFile to Hadoop Distributed File System (hdfs). Given the custom attribute \"fileFormat\" we have created before for each FlowFile, we can use this attribute as an output path for each FlowFile in HDFS.\n",
    "\n",
    "|Configuration|Description|Value in the example|\n",
    "|--|-----|-----|\n",
    "|Hadoop Configuration Resources|A file or comma separated list of files which contains the Hadoop file system configuration. We usually put the path of core-site.xml file here|/home/yueliu/Documents/hadoop-2.8.1/etc/hadoop/core-site.xml|\n",
    "|Directory|The parent HDFS directory to which files should be written. The directory will be created if it does not exist.|/sample/${fileFormat}|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Check within HDFS\n",
    "\n",
    "\n",
    "Use the following code in terminal to check the result of the above NiFi dataflow:\n",
    "```\n",
    "hadoop fs -ls /sample/\n",
    "```\n",
    "Here is the result of the above two NiFi dataflows:\n",
    "![egauge_portfolio](img/nifi_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advantages of Apache NiFi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through answering the following questions, we will address the advantages of NiFi and show why the pipeline can potentially contribute to PERCEIVE project in the future:\n",
    "\n",
    "- **Is the raw data still there?**\n",
    "\n",
    "    It depends on the configuration of processors. For example, user can define the contribution of GetFile processor and set \"false\" for \"Keep source file\" property, thus keeping the raw data within the input directory. \n",
    "\n",
    "\n",
    "- **Is the processed data now sitting in HDFS processed?**\n",
    "   \n",
    "   Yes. Through the above screenshot of HDFS, all processed data are sitting within month subfolders in HDFS. \n",
    "    \n",
    "    \n",
    "- **How to add multiple files?**\n",
    "   \n",
    "   If all files are stored in one directory, it only needs one Get processor to retrieve these files. If files are stored separately or stored in different systems, it might need multiple Get processors to generate files and then process. \n",
    "    \n",
    "    \n",
    "- **If we do have multiple files, are we running in parallel?**\n",
    "\n",
    "    In GetFile processor, there is a property named \"batch size\" that let user to define the maximum number of files to pull in each iteration. For multiple Get processors, they will be running in parallel if user run them at the same time. For example, in the first picture, GetFile processor and GetHTTP processor are running at the same time if we run them together.    \n",
    "    \n",
    "    \n",
    "- **What do I need to do to run NiFi?**\n",
    "    \n",
    "    NiFi only needs Java environment to run.\n",
    "    \n",
    "    \n",
    "- **What is going on after I click to execute the NiFi Pipeline?**\n",
    "\n",
    "    From the high level, FlowFile will be passed through multiple processors, routed to different flows based on the configuration of processors, analyzed/processed, and then sent to local systems or other big data products. \n",
    "    \n",
    "    \n",
    "- **NiFi best selling point is data provenance. What can we gain by using that versus just doing everything with NiFi? What are the pros? What are the cons? Is it worthwhile the overhead of a new project?**\n",
    "\n",
    "    \"NiFi keeps a very granular level of detail about each piece of data that it ingests. As the data is processed through the system and is transformed, routed, split, aggregated, and distributed to other endpoints, this information is all stored within NiFi’s Provenance Repository\"\". Through \"Data Provenance\" from the Global Menu, there is a table to list the provenance events that track each FlowFile. Also,as shown below, NiFi can track how one specific FlowFile is processed through data flow.\n",
    "    \n",
    "    ![egauge_portfolio](img/nifi_dataprovenance_1.png)\n",
    "    \n",
    "    ![egauge_portfolio](img/nifi_dataprovenance_2.png)\n",
    "    \n",
    "    As introduced at the beginning, NiFi is a dataflow tool that is compatible with multiple big data products and cloud systems. Therefore, through using NiFi, it can easily represent the big picture of how data are processed through complex analysis, process, split, aggregation, interaction, and distribution with other products. Therefore, it is much easy to manage and make a change for the overall processing. In other words, the big advantage of NiFi is that the analysis procedures are visualized and thus can be used for multiple times. Therefore, NiFi is more suitable for the projects that have streaming data. If the set of processing procedures within NiFi is used for only one time, it may waste more time on building NiFi processors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference \n",
    "[1] HDF/NiFi to convert row-formatted text files to columnar Parquet and ORC: https://community.hortonworks.com/articles/70257/hdfnifi-to-convert-row-formatted-text-files-to-col.html\n",
    "\n",
    "[2] Stream Processing: NiFi and Spark https://blogs.apache.org/nifi/entry/stream_processing_nifi_and_spark\n",
    "\n",
    "[3]NiFi data provence: https://blogs.apache.org/nifi/entry/basic_dataflow_design\n",
    "\n",
    "[4] Nifi common attribute: https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_getting-started-with-apache-nifi/content/common-attributes.html\n",
    "\n",
    "[5] NiFi Expression Language and function: https://docs.hortonworks.com/HDPDocuments/HDF3/HDF-3.0.0/bk_getting-started-with-apache-nifi/content/ExpressionLanguage.html\n",
    "\n",
    "[6] Converting CSV To Avro with Apache NiFi: https://community.hortonworks.com/articles/28341/converting-csv-to-avro-with-apache-nifi.html\n",
    "\n",
    "[7] Stream data into HIVE like a Boss using NiFi HiveStreaming - Olympics 1896-2008: https://community.hortonworks.com/articles/52856/stream-data-into-hive-like-a-king-using-nifi.html\n",
    "\n",
    "[8] Using NiFi to ingest and transform RSS feeds to HDFS using an external config file: https://community.hortonworks.com/articles/48816/nifi-to-ingest-and-transform-rss-feeds-to-hdfs-usi.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
