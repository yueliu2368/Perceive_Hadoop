{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief introduction to TopicFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TopicFlow is a tool that visualizes the results of automatic topic detection and topic alignment between sets of tweets over time. The tool was developed by Jianyu Li, Sana Malik, Panagis (Pano) Papadatos and Alison Smith originally as a team project for CMSC 734 Information Visualization at the University of Maryland. You can find more information about TopicFlow by reading the README.md and their papers:\n",
    "- [TopicFlow: Visualizing Topic Alignment of Twitter Data over Time](https://wiki.cs.umd.edu/cmsc734_f12/images/0/05/TopicFlowFinalReport2.pdf)\n",
    "- [Visual Analysis of Topical Evolution in Unstructured Text: Design and Evaluation of TopicFlow](http://link.springer.com/chapter/10.1007/978-3-319-19003-7_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to achieve by utilizing TopicFlow for [PERCEIVE](https://github.com/sailuh/perceive) is trying to visualize the \"flow\" of topics of Full Disclosure documents that may help us identify upcoming cybersecurity threats. \n",
    "\n",
    "PERCEIVE is developed and maintained by a joint effort of many contributors. The role of TopicFlow in PERCEIVE can be simplified with the graph below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![work flow](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/work%20flow.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output of TopicFlow pipeline is the visualization, the output of this data transformation pipeline is a *run.py* file that enables a user to create new TopiFlow projects or run an existing project. \n",
    "\n",
    "Although TopicFlow is a powerful tool, it was designed to visualize only the flow of \"tweets\". To make TopicFlow work for Full Disclosure data, several changes were made to the original scripts:\n",
    "1. changed all \"tweet\" related content to \"doc\" or \"document\" in the final visualization;\n",
    "2. disabled \n",
    "```javascript\n",
    "if ($(\"g #\"+j)[0].style.display != \"none\") { }\n",
    "``` \n",
    "in *controller.js* to avoid `style errors`. Otherwise, TopicFlow couldn't configure text data other than tweets.\n",
    "3. removed all datasets to select from except Full Disclosure 2012 dataset. Several changes were made in *index.html*, *controller.js*, and */topicflow/data* directory. For example, in *controller.js*, the original version allows users to choose some of these datasets:\n",
    "```javascript\n",
    "var idToName = {\"HCI\" : \"HCI\", \"ModernFamily\" : \"Modern Family\", \"catfood\": \"Catfood\" , \n",
    "\t\t\t\t\t\"drugs\" : \"Drugs\", \"earthquake\" : \"Earthquake\", \"umd\" : \"UMD\", \"debate\":\"#debate\", \"chi\":\"CHI Conference\", \n",
    "\t\t\t\t\t\"sandy\" : \"Sandy and NJ\"}\n",
    "```\n",
    "however, in our final version, we only need the following dataset as the starting point:\n",
    "```javascript\n",
    "var idToName = {\n",
    "                // add new idToName\n",
    "                \"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So How do we approach this? After exploring TopicFlow and discussing with Carlos Paradis several times, I believe the best way to utilize TopicFlow without overhauling the original codes is modifying only the parts that help us display our datasets. Since TopicFlow is pretty hand-coded, this data transformation pipeline has to edit the actual scripts and generate new files with our Python program. \n",
    "\n",
    "To better understand what we need to do. Here let's take a look at how TopicFlow works in the simpliest form, a triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![methodology](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/methodology.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially two scripts and one data directory controls how TopicFlow works: *index.html* provides the place for the visualization and the basic information, */data/< project >* stores the actual data to display, and *controller.js* coordinates all the JavaScript scripts and tells TopicFlow how to read data and the way to visualize. The highlighted elements are what we will be modifying or creating in this data transformation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that this transformation pipeline only works for Full Disclosure data**\n",
    "\n",
    "The functions in this pipeline only works for PERCEIVE datasets. To create a new project, make sure you have the following four directories that have the listed structure: \n",
    "    \n",
    "*path_tf (path of TopicFlow)*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- css  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- data  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- scripts  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- index.html  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- run.py  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- ......  \n",
    "\n",
    "*path_doc (path of parsed documents directory)*   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- yyyy_mm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- yyyy_mm_relative_id.extension\n",
    "\n",
    "*path_meta (path of metadata of parsed documents directory)*   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- yyyy_mm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- yyyy_mm.csv   \n",
    "\n",
    "*path_LDA  (path of LDA directory)*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Document_Topic_Matrix  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- mm.csv    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Topic_Flow  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- topic_flow.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Topic_Term_Matrix  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- mm.csv    \n",
    "\n",
    "\n",
    "**Example of 2014 Full Diclosure datasets**\n",
    "\n",
    "*/&ast;&ast;/Topicflow*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- css  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- data  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- scripts  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- index.html  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- run.py  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- ......  \n",
    "\n",
    "*/&ast;&ast;/2014.parsed*   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_01    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Jan_0.reply.body.txt  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Jan_0.reply.body_no_signature.txt  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Jan_1.reply.body.txt  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Jan_1.reply.body_no_signature.txt  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- ......  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_02  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- ......  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- ......    \n",
    "\n",
    "*/&ast;&ast;/2014.csv*   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_01    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Jan.csv    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_02    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- 2014_Feb.csv    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- ......    \n",
    "\n",
    "*/&ast;&ast;/2014_k_10*  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Document_Topic_Matrix  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- Jan.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- Feb.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- ......  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Topic_Flow  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- topic_flow.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;|- Topic_Term_Matrix   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- Jan.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- Feb.csv  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|- ......  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walking Through All Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I'll try to explain how each data transformation functon works in a language that's easy to comprehend. The flow of `run.py` looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run.py flow](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/run.py%20flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argparse and local server will be covered in Function 7. Another function not mentioned in this notebook is **read_data**, which just reads data and store them as pandas.DataFrame objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: In TopicFlow directory, a backup of the original *index.html*, *scripts*, and *data* is included in */topicflow_backup*. Use it to restore missing *index.html* and *controller.js* if something went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1 - transform_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with reading all text files. Functions **transform_doc**, **transform_bins**, and **transform_topicSimilarity** will load the necessary datasets that the user intends to visualize in TopicFlow, transform the data into the format that TopicFlow can read, and create a JavsScript file in the new project data directory.\n",
    "\n",
    "In order to transform data, I think it's worth spending some time doing reverse engineering. Let's first understand what's the end result of **transform_doc** and how it works. The end result is a file called *Doc.js* inside the project data directory. Say the name of the new project is \"FD2014\", the path of the end result would be `/topicflow/data/FD2014/Doc.js`. *Doc.js* is essentially a JavaScript function that contains all the document text and the metadata of the document, and calling another function defined in *controller.js* to read the data. The skeleton of *Doc.js* looks like:\n",
    "```javascript\n",
    "function populate_tweets_FD2014(){\n",
    "    var tweet_data ={\"1\":{\"tweet_id\":1,\"author\":...,\"tweet_date\":...,\"text\":...}, \"2\":...\n",
    "    readTweetJSON(tweet_data);\n",
    "}\n",
    "```\n",
    "If you open it for the first time, the length of this file would be daunting, but it actually has a very simple structure. First, a JavaScript function called **populate_tweets_FD2014** (\"FD2014\" is the project name) is defined. Then, a variable called \"tweet_data\" is defined, along with literally all the document data in JSON format as the value of this variable. At last, the function **readTweetJSON** defined in *controller.js* is called to actually read the data in \"tweet_data\" variable. \n",
    "\n",
    "One thing important to clarify here is the word \"tweet\", or \"tweets\". Although we are utilizing TopicFlow to read data other than tweets, the file names and function names in TopicFlow inherit the nature of the initial purpose by putting \"tweet\" or \"tweets\" in them. There are so many functions and codes in different files having \"tweet\" and they are so interwined that I couldn't alter this naming convention at this stage. But luckily we can name this file as *Doc.js* instead of *Tweet.js*. Hooray!\n",
    "\n",
    "Okay, now let's see what the JSON part in *Doc.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"1\": {\n",
    "    \"tweet_id\": 1,\n",
    "    \"author\": \"Luciano Bello <luciano () debian org>\",\n",
    "    \"tweet_date\": \"12\\/31\\/2013 16:46\",\n",
    "    \"text\": \"...\"\n",
    "    },\n",
    "  \"2\": {\n",
    "    ...\n",
    "    },\n",
    "  ...\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first initialize two dictionaries, one storing our document data (mainly in .txt files) and required metadata, and the other dictionary storing the document ids and names of matched text file. The second dictionary will be used in **transform_bins**. Aftering transforming the first big dictionary into JSON format, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *Doc.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_doc](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/transform_doc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_doc(project_name, path_doc, path_meta, doc_extension):\n",
    "    \"\"\"\n",
    "    Transform Full Disclosure email documents from .txt formats into\n",
    "    JavaScript format that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_doc     -- path of documents directory\n",
    "\n",
    "    Returns:\n",
    "        a dictionary that maps document id with .txt file name that will be \n",
    "        used in transform_bins\n",
    "        \n",
    "    Outcome:        \n",
    "        \"Doc.js\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### READ METADATA\n",
    "    df_list = read_data(df_list=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION\n",
    "    # initiate one main dictionary of Doc.js and one dictionary that maps \n",
    "    # document id with .txt file name\n",
    "    tweet_data = {}    # which contains all elements of documents\n",
    "    tweet_id_txt = {}  # use this for transform_bins\n",
    "    \n",
    "    # find documents\n",
    "    id_pointer = 1     # tweet_id starts with 1\n",
    "    for month_ix, folder in enumerate(os.listdir(path_doc)):\n",
    "        tweet_id_txt[str(month_ix)] = {}\n",
    "        tweet_id_txt[str(month_ix)]['id'] = []\n",
    "        tweet_id_txt[str(month_ix)]['txt'] = []\n",
    "        path_folder = os.path.join(path_doc, folder)\n",
    "        # read .txt files with the user-specified extension\n",
    "        txt_list = [x for x in os.listdir(path_folder) if x.endswith(doc_extension)]\n",
    "        # find .txt files that match their metadata entries\n",
    "        for txt in txt_list:\n",
    "            txt_entry_elements = txt.split('.')[0].split('_') # looks like ['2005', 'Jan', '0']\n",
    "            txt_entry_elements[1] = folder[-2:]               # looks like ['2005', '01', '0']\n",
    "            txt_entry = '_'.join(txt_entry_elements)          # looks like '2005_01_0', use this to find document metadata in .csv file\n",
    "            # only make a record if there's a match between .txt file and metadata,\n",
    "            # and the file is readable.\n",
    "            try:\n",
    "                row = df_list[month_ix][df_list[month_ix]['id'] == txt_entry]\n",
    "                author = row['author'].values[0]\n",
    "                date = pd.to_datetime(row['date']).apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).values[0]\n",
    "                with open(os.path.join(path_folder, txt), 'r',\n",
    "                          encoding='latin1') as textfile:     # notice the encoding\n",
    "                    text = textfile.read().replace('\"','').replace('http://','').replace('\\\\','').replace('\\n','') # remove irrgular expressions\n",
    "                \n",
    "                # populate content\n",
    "                tweet_data[str(id_pointer)] = {}\n",
    "                tweet_id_txt[str(month_ix)]['id'].append(id_pointer)\n",
    "                tweet_id_txt[str(month_ix)]['txt'].append(txt.split('.')[0] + '.txt')\n",
    "                tweet_data[str(id_pointer)]['tweet_id'] = id_pointer\n",
    "                tweet_data[str(id_pointer)]['author'] = author\n",
    "                tweet_data[str(id_pointer)]['tweet_date'] = date\n",
    "                tweet_data[str(id_pointer)]['text'] = text\n",
    "                \n",
    "                id_pointer += 1\n",
    "            # for any reason the above try fails, we don't record\n",
    "            except:\n",
    "                # here, you can do things like listing files that can't be parsed\n",
    "                # e.g. print(txt)\n",
    "                pass\n",
    "                \n",
    "    # transform body into .json format\n",
    "    json_tmp = json.dumps(tweet_data)\n",
    "\n",
    "    # transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_tweets_' + project_name + '(){\\nvar tweet_data ='\n",
    "    posfix = ';\\nreadTweetJSON(tweet_data);\\n}'\n",
    "    doc_js = prefix + json_tmp + posfix\n",
    "\n",
    "    ### WRITE\n",
    "    # make a directory named after project_name\n",
    "    if os.path.isdir(os.path.join(path_tf, 'data', project_name)) == False:\n",
    "        os.mkdir(os.path.join(path_tf, 'data', project_name))\n",
    "        \n",
    "    # write\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'Doc.js'), 'w') as file:\n",
    "        file.write(doc_js)\n",
    "\n",
    "    print('\\nDoc.js created,             20% complete.')\n",
    "    \n",
    "    return tweet_id_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"Doc.js created,             20% complete.\" will be printed out in the terminal. This newly created file should populate the document content on the right side of TopicFlow. Clicking a document should let a uer see the author, date, and actual text of that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2 - transform_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_bins** is the hardest part in the whole data transformation pipeline. Although it has the same three-part-structure as **transform_doc**, the JSON part in **transform_bins** is much more complex, thus require very careful handling of indexing and putting data in the right place. Here we can take a quick glance of the model that's draw by Carlos Paradis:\n",
    "![bins_model](https://raw.githubusercontent.com/estepona/topicflow/master/data_model/bins_model.png)\n",
    "\n",
    "Again, let's do reverse engineering. The end result is a file called *Bins.js* inside the project data directory. Say the name of the new project is \"FD2014\", the path of the end result would be `/topicflow/data/FD2014/Bins.js`. *Bins.js* is essentially a JavaScript function that divide all documents by time (in the example of Full Disclosure data, divide by month) which is called binning, and store the LDA data (document-topic scores and topic-word scores) of all the pairs. The skeleton of *Bins.js* looks like:\n",
    "```javascript\n",
    "function populate_bins_FD2014(){\n",
    "    var bin_data ={\"0\":{\"tweet_Ids\":[...],\"start_time\":...,\"bin_id\":...,\"topic_model\":{...},\"end_time\":...},\"1\":...\n",
    "    readBinJSON(bin_data);\n",
    "}\n",
    "```\n",
    "Again, it's could be daunting the first time you open it: it's very lengthy, but the structure stays the same. First, a JavaScript function called **populate_bins_FD2014** (\"FD2014\" is the project name) is defined. Then, a variable called \"bin_data\" is defined, along with all the relevent data in JSON format as the value of this variable. At last, the function **readBinJSON** defined in *controller.js* is called to read the data in \"bin_data\" variable. \n",
    "\n",
    "Now let's see what the JSON part in *Bins.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"0\": {\n",
    "    \"tweet_Ids\": [1,2,3...],\n",
    "    \"start_time\": \"12/31/2013 16:46\",\n",
    "    \"bin_id\": 0,\n",
    "    \"topic_model\": {\n",
    "            \"topic_doc\": {\n",
    "                    \"0_0\": {\n",
    "                        \"1\": 0.00010030434072387,\n",
    "                        \"2\": 0.36551017173243494,\n",
    "                        ...\n",
    "                    },\n",
    "                    \"0_1: {...},\n",
    "                    ...\n",
    "                },\n",
    "            \"doc_topic\": {\n",
    "                \"1\": {\n",
    "                    \"0_0\": 0.00010030434072387,\n",
    "                    \"0_1\": 0.00010030434072383,\n",
    "                    ...\n",
    "                },\n",
    "                \"2\": {...},\n",
    "                ...\n",
    "            },\n",
    "            \"topic_word\": {\n",
    "                \"0_0\": {\n",
    "                    \"x86_64\": 0.0361921097895964,\n",
    "                    \"i586\": 0.0335562698609424,\n",
    "                    ...\n",
    "                },\n",
    "                \"0_1\": {...},\n",
    "                ...\n",
    "            },\n",
    "            \"topic_prob\": {\n",
    "                \"0\": \"0_0\",\n",
    "                \"1\": \"0_1\",\n",
    "                ...\n",
    "            }\n",
    "        },\n",
    "    \"end_time\": \"1/31/2014 21:25\"\n",
    "    },\n",
    "  \"1\": {\n",
    "    ...\n",
    "  },\n",
    "  ...\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first process and store all the data in a dictionary, and transform it into JSON format. Then, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *Bins.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_bins](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/transform_bins.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of how **transform_bins** works can be found in the comments. One thing to notice is that this function takes more consideration in indexing than other functions because there are so many document-topic and topic-word pairs to populate and sometimes the index starts with 0 and sometimes it starts with 1, a consistancy issue that's hard to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_bins(project_name, path_doc, path_meta, path_LDA, tweet_id_txt):\n",
    "    \"\"\"\n",
    "    Transform LDA-genereted Topic-document matrixes and Topic-word\n",
    "    matrixes into JavaScript format that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_doc     -- path of documents directory\n",
    "        path_LDA     -- path of LDA main directory, this directory should\n",
    "                        contain 3 sub-directories: Document_Topic_Matrix,\n",
    "                        Topic_Flow, and Topic_word_Matrix\n",
    "        tweet_id_txt -- a dictionary that maps document id with .txt file name\n",
    "                        generated by transform_doc     \n",
    "\n",
    "    Outcome:\n",
    "        \"Bins.js\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINE month_list, READ DATA\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    # read df_list\n",
    "    df_list = read_data(df_list=True)\n",
    "    # read topic-doc & topic-word data sets\n",
    "    df_topic_doc = read_data(df_topic_doc=True)\n",
    "    # read topic-word data sets\n",
    "    df_topic_word = read_data(df_topic_word=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION - 1\n",
    "    # initiate bins, each month is one bin, each bin is also a dictionary\n",
    "    bin_dict = {}\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)] = {}\n",
    "\n",
    "    # populate bin_id\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['bin_id'] = month_ix\n",
    "\n",
    "    # populate tweet_ids\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['tweet_Ids'] = tweet_id_txt[str(month_ix)]['id']\n",
    "\n",
    "    # populate start_time & end_time\n",
    "    # here we need input from df_list, specifically the length of each month\n",
    "    # this part sorts out the earliest and latest time of a tweet in each month, and\n",
    "    # transform them into \"mm/dd/yy hh:mm\" format\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['start_time'] = pd.to_datetime(df_list[month_ix].date).sort_values().apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).tolist()[0]\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['end_time'] = pd.to_datetime(df_list[month_ix].date).sort_values().apply(lambda x: str(x.month) + '/' + str(x.day) + '/' + str(x.year) + ' ' + str(x.hour) + ':' + str(x.minute)).tolist()[-1]\n",
    "\n",
    "    # initiate topic_model\n",
    "    for month_ix in range(len(month_list)):\n",
    "        bin_dict[str(month_ix)]['topic_model'] = {}\n",
    "        # add 4 sub dictionaries\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_doc'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['doc_topic'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_word'] = {}\n",
    "        bin_dict[str(month_ix)]['topic_model']['topic_prob'] = []\n",
    "\n",
    "        \n",
    "    ### DATA TRANSFORMATION - 2: POPULATE topic_model\n",
    "    # topic_model is the hardest part. We need to populate them month by month,\n",
    "    # and one by one.\n",
    "    for month_ix in range(len(month_list)):\n",
    "        overlap = set(df_topic_doc[month_ix].index.tolist()) & set(tweet_id_txt[str(month_ix)]['txt'])\n",
    "        overlap = list(overlap)\n",
    "        df_topic_doc_overlap = df_topic_doc[month_ix].copy().loc[overlap, :]\n",
    "        \n",
    "        # topic_prob & topic_doc\n",
    "        for prob in range(10):\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_prob'].append(str(month_ix) + '_' + str(prob))\n",
    "            # initiate topic_doc\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_doc'][str(month_ix) + '_' + str(prob)] = {}\n",
    "            overlap_id = [tweet_id_txt[str(month_ix)]['txt'].index(index_txtfile) for index_txtfile in df_topic_doc_overlap.index.tolist()]\n",
    "            overlap_id = [tweet_id_txt[str(month_ix)]['id'][index_tweetid] for index_tweetid in overlap_id]\n",
    "            for overlap_ix in range(len(overlap_id)):\n",
    "                bin_dict[str(month_ix)]['topic_model']['topic_doc'][str(month_ix) + '_' + str(prob)][str(overlap_id[overlap_ix])] = df_topic_doc_overlap[str(int(prob + 1))].tolist()[overlap_ix]\n",
    "        \n",
    "        # doc_topic\n",
    "        overlap_id = [tweet_id_txt[str(month_ix)]['txt'].index(index_txtfile) for index_txtfile in df_topic_doc_overlap.index.tolist()]\n",
    "        overlap_id = [tweet_id_txt[str(month_ix)]['id'][index_tweetid] for index_tweetid in overlap_id] \n",
    "        for overlap_ix2 in range(len(overlap_id)):\n",
    "            row = df_topic_doc_overlap.iloc[overlap_ix2, :].tolist()\n",
    "            bin_dict[str(month_ix)]['topic_model']['doc_topic'][str(overlap_id[overlap_ix2])] = {}\n",
    "            for row_ix in range(len(row)):\n",
    "                bin_dict[str(month_ix)]['topic_model']['doc_topic'][str(overlap_id[overlap_ix2])][str(month_ix) + '_' + str(row_ix)] = row[row_ix]\n",
    "            \n",
    "        # topic_word\n",
    "        for topic_word_ix in range(10):\n",
    "            name = str(month_ix) + '_' + str(topic_word_ix)\n",
    "            bin_dict[str(month_ix)]['topic_model']['topic_word'][name] = {}\n",
    "            topwords = df_topic_word[month_ix].iloc[topic_word_ix].sort_values(ascending=False)[:10]\n",
    "            topwords = np.around(topwords, 17)\n",
    "            # we choose top 10 most frequent words, so here the range is 10\n",
    "            for topword_ix in range(10):\n",
    "                bin_dict[str(month_ix)]['topic_model']['topic_word'][name][topwords.index[topword_ix]] = topwords.values[topword_ix]\n",
    "        \n",
    "        # delete df_topic_doc_overlap to aviod overwritting error and save memory\n",
    "        del df_topic_doc_overlap\n",
    "        \n",
    "\n",
    "    ### TRANSFORM INTO JS FORMAT\n",
    "    # transform bin_dict into an ordered dictionary\n",
    "    bin_dict_ordered = {}\n",
    "\n",
    "    key_order = ('tweet_Ids','start_time','bin_id','topic_model','end_time')\n",
    "    for month_ix in range(len(month_list)):\n",
    "        tmp = OrderedDict()\n",
    "        for k in key_order:\n",
    "            tmp[k] = bin_dict[str(month_ix)][k]\n",
    "        bin_dict_ordered[str(month_ix)] = tmp\n",
    "\n",
    "    # transform body into .json format\n",
    "    json_tmp = json.dumps(bin_dict_ordered)\n",
    "\n",
    "    # transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_bins_' + project_name + '(){\\nvar bin_data = '\n",
    "    posfix = ';\\nreadBinJSON(bin_data);\\n}'\n",
    "    bins_js = prefix + json_tmp + posfix\n",
    "\n",
    "\n",
    "    ### WRITE\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'Bins.js'), 'w') as file:\n",
    "        file.write(bins_js)\n",
    "\n",
    "    print('Bins.js created,            40% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"Bins.js created,            40% complete.\" will be printed out in the terminal. This newly created file should populate the both the bottom-left and center area of TopicFlow. Each column in the visualization is a bin and each box is a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3 - transform_topicSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After bins and topics are created, **transform_topicSimilarity** generates nodes and links between topics in adjacent bins. It also has the three-part-structure as **transform_doc** and **transform_bins**. \n",
    "\n",
    "Reverse engineering! The end result is a file called *TopicSimilarity.js* inside the project data directory. Say the name of the new project is \"FD2014\", the path of the end result would be `/topicflow/data/FD2014/TopicSimilarity.js`. *TopicSimilarity.js* is essentially a JavaScript function that scores how similar the topics between two adjancent bins are. The scores are also generated by the LDA algorithm. The skeleton of *Bins.js* looks like:\n",
    "```javascript\n",
    "function populate_similarity_FD2014(){\n",
    "    var sim_data ={\"nodes\":[{\"name\":...,\"value\":...},...],\"links\":[{\"source\":...,\"target\":...,\"value\":...},...]}\n",
    "    readSimilarityJSON(sim_data);\n",
    "}\n",
    "```\n",
    "*TopicSimilarity.js* is the shortest among all three data files and it follows a simple logic: we have nodes and score the links between nodes. As of the overall JavaScript structure, first, a function called **populate_similarity_FD2014** (\"FD2014\" is the project name) is defined. Then, a variable called \"sim_data\" is defined, along with all the relevent data in JSON format as the value of this variable. At last, the function **readSimilarityJSON** defined in *controller.js* is called to read the data in \"sim_data\" variable. \n",
    "\n",
    "Now let's see what the JSON part in *TopicSimilarity.js* looks like:\n",
    "```json\n",
    "{\n",
    "  \"nodes\": [\n",
    "      {\n",
    "          \"name\": \"0_0\",\n",
    "          \"value\": 43\n",
    "      },\n",
    "      {\n",
    "          \"name\": \"0_1\",\n",
    "          \"value\": 57\n",
    "      },\n",
    "      ...\n",
    "  ],\n",
    "  \"links\": [\n",
    "      {\n",
    "          \"source\":1,\n",
    "          \"target\":18,\n",
    "          \"value\":233.6647080989732\n",
    "      },\n",
    "      {\n",
    "          \"source\":2,\n",
    "          \"target\":13,\n",
    "          \"value\":183.70069470814772\n",
    "      },\n",
    "      ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "To make the data transformation work, we have to first process and store all the similarity data in a dictionary, and transform it into JSON format. Then, we can add the codes before and after the JSON part with one customization on the project name. Finally, write to *TopicSimilarity.js*. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![transform_topicSimilarity](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/transform_topicSimilarity.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_topicSimilarity(project_name, path_LDA):\n",
    "    \"\"\"\n",
    "    Transform topic similarity matrix into JavaScript format\n",
    "    that TopicFlow can read.\n",
    "\n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_LDA     -- path of LDA main directory, this directory should\n",
    "                        contain 3 sub-directories: Document_Topic_Matrix,\n",
    "                        Topic_Flow, and Topic_word_Matrix\n",
    "\n",
    "    Outcome:\n",
    "        \"TopicSimilarity.js\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### DEFINE month_list, READ DATA\n",
    "    month_list = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    df_topic_sim = read_data(df_topic_sim=True)\n",
    "\n",
    "\n",
    "    ### DATA TRANSFORMATION\n",
    "    # initiate a dictionary\n",
    "    sim_dict = {}\n",
    "\n",
    "    # populate nodes\n",
    "    # put topics into nodes, record their orders\n",
    "    nodes = []\n",
    "    for i in range(len(month_list)):\n",
    "        for j in range(10):\n",
    "            tmp = {}\n",
    "            name = str(i) + '_' + str(j)\n",
    "            # how to calculate the value of a topic? the paper didn't define clearly\n",
    "            # so here I use a random number\n",
    "            value = np.random.randint(1,100)\n",
    "            tmp['name'], tmp['value'] = name, value\n",
    "            nodes.append(tmp)\n",
    "\n",
    "    # populate links\n",
    "    # put source, target, value into links\n",
    "    links = []\n",
    "    for month_ix in range(len(month_list) - 1):\n",
    "        # get unique pais between every two months, in total we have 11 pairs\n",
    "        mm1, mm2 = month_list[month_ix], month_list[month_ix + 1]\n",
    "        sim = mm1 + '_' + mm2 + '_similarity'\n",
    "        df_tmp = df_topic_sim[[mm1, mm2, sim]].dropna(axis=0).drop_duplicates()\n",
    "        for row_ix in range(len(df_tmp)):\n",
    "            source = month_ix*10 + int(df_tmp[mm1].values[row_ix]) - 1\n",
    "            target = (month_ix+1)*10 + int(df_tmp[mm2].values[row_ix]) - 1\n",
    "            score = df_tmp[sim].values[row_ix] * 200 # 200 makes it neither too thin nor too thick\n",
    "            link_tmp = {}\n",
    "            link_tmp['source'], link_tmp['target'], link_tmp['value'] = source, target, score\n",
    "            links.append(link_tmp)\n",
    "\n",
    "    # put two lists into sim_dict\n",
    "    sim_dict['nodes'], sim_dict['links'] = nodes, links\n",
    "\n",
    "\n",
    "    ### TRANSFORM INTO JS FORMAT\n",
    "    json_tmp = json.dumps(sim_dict)\n",
    "\n",
    "    # finally, transform into .js format that TopicFlow can read\n",
    "    prefix = 'function populate_similarity_' + project_name + '(){\\nvar sim_data = '\n",
    "    posfix = ';\\nreadSimilarityJSON(sim_data);\\n}'\n",
    "    topicSimilarity_js = prefix + json_tmp + posfix\n",
    "\n",
    "\n",
    "    ### WRITE\n",
    "    with open(os.path.join(path_tf, 'data', project_name, 'TopicSimilarity.js'), 'w') as file:\n",
    "        file.write(topicSimilarity_js)\n",
    "\n",
    "    print('TopicSimilarity.js created, 60% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"TopicSimilarity.js created, 60% complete.\" will be printed out in the terminal. This newly created file should control the top-left panel of TopicFlow and the lines between different topics. These data are in charge of topic flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4 - modify_html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the *index.html* file in TopicFlow controls the loading of datasets and display the dataset selector when user initiates TopicFlow or changes datasets. Respectively, the two parts in index.html looks like:\n",
    "```html\n",
    "<script src=\"data/Full_Disclosure_2012/Tweet.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/Bins.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/TopicSimilarity.js\"></script>\n",
    "\n",
    "<!-- add new section after this line -->\n",
    "<!-- end of adding new datasets. -->\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```html\n",
    "<li id=\"Full_Disclosure_2012\"><a href=\"#\">Full_Disclosure_2012</a></li>\n",
    "<!-- add new dataset selector after this line -->\n",
    "<!-- end of adding new dataset selector -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let the function find the locations of the above parts and add codes for a new dataset in the same style. To make it faster finding the locations, four lines of comments are placed so that the program easily finds the place for our insertion. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modify_html](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/modify_html.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_html(project_name, path_tf):\n",
    "    \"\"\"\n",
    "    Modify the content of \\topicflow\\index.html.\n",
    "    Two hand-added comments are used to locate the lines where new content can be\n",
    "    added. Executing the function would replace the existing index.html.\n",
    "    \n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_tf      -- path of topicflow directory\n",
    "    \n",
    "    Outcome:\n",
    "        a modified \"index.html\" that includes a new project\n",
    "    \"\"\"\n",
    "    # read exisitng index.html and parse by lines\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'r') as file:\n",
    "        html = file.read()\n",
    "\n",
    "    html_parse = html.split('\\n')\n",
    "\n",
    "    # add new section after '<!-- add new section after this line -->'\n",
    "    ix = html_parse.index('<!-- add new section after this line -->')\n",
    "    new_section = '<script src=\"data/SHA/Doc.js\"></script>\\n<script src=\"data/SHA/Bins.js\"></script>\\n<script src=\"data/SHA/TopicSimilarity.js\"></script>\\n'.replace('SHA',project_name)\n",
    "    html_parse.insert(ix+1, new_section)\n",
    "\n",
    "    # add new selector after '<!-- add new dataset selector after this line -->'\n",
    "    ix = html_parse.index('\\t\\t\\t<!-- add new dataset selector after this line -->')\n",
    "    new_selector = '\\t\\t\\t<li id=\"SHA\"><a href=\"#\">SHA</a></li>'.replace('SHA', project_name)\n",
    "    html_parse.insert(ix+1, new_selector)\n",
    "\n",
    "    # replace existing index.html\n",
    "    html_combine = '\\n'.join(html_parse)\n",
    "    os.remove(os.path.join(path_tf, 'index.html'))\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'w') as file:\n",
    "        file.write(html_combine)\n",
    "\n",
    "    print('index.html modified,        80% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"index.html modified,        80% complete.\" will be printed out in the terminal. The new index.html should have the following changes being made. In this example, the new project is called \"FD2014\":\n",
    "```html\n",
    "<script src=\"data/Full_Disclosure_2012/Tweet.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/Bins.js\"></script>\n",
    "<script src=\"data/Full_Disclosure_2012/TopicSimilarity.js\"></script>\n",
    "\n",
    "<!-- add new section after this line -->\n",
    "<script src=\"data/FD2014/Doc.js\"></script>\n",
    "<script src=\"data/FD2014/Bins.js\"></script>\n",
    "<script src=\"data/FD2014/TopicSimilarity.js\"></script>\n",
    "<!-- end of adding new datasets. -->\n",
    "```\n",
    "and\n",
    "```html\n",
    "<li id=\"Full_Disclosure_2012\"><a href=\"#\">Full_Disclosure_2012</a></li>\n",
    "<!-- add new dataset selector after this line -->\n",
    "<li id=\"FD2014\"><a href=\"#\">FD2014</a></li>\n",
    "<!-- end of adding new dataset selector -->\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5 - modify_controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same methodology as **modify_html**, **modify_controller** locates two parts in *controller.js* that controls how TopicFlow reads the data of our new project and what functions to call to parse the data. Respectively, the two parts nested in the function **populateVisualization** in *controller.js* look like:\n",
    "```javascript\n",
    "var idToName = {\n",
    "                // add new idToName\n",
    "                \"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```\n",
    "and\n",
    "```javascript\n",
    "// Populate the interface with the selected data set\n",
    "if (selected_data===\"Full_Disclosure_2012\") {\n",
    "    populate_tweets_Full_Disclosure_2012();\n",
    "    populate_bins_Full_Disclosure_2012();\n",
    "    populate_similarity_Full_Disclosure_2012();\n",
    "}\n",
    "// add new selected dataset here\n",
    "// end of adding new selected datasets\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let the function find the locations of the above parts and add codes for a new idToName variable and a new selected dataset in the same style. To make it faster finding the locations, three lines of comments are placed so that the program easily finds the place for our insertion. The overall flow looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modify_controller](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/modify_controller.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modify_controller(project_name, path_tf):\n",
    "    \"\"\"\n",
    "    Modify the content of \\topicflow\\scripts\\controller.js.\n",
    "    Two hand-added comments are used to locate the lines where new content can be\n",
    "    added. Executing the function would replace the existing controller.js.\n",
    "    \n",
    "    Args:\n",
    "        project_name -- name of the new project\n",
    "        path_tf      -- path of topicflow directory\n",
    "    \n",
    "    Outcome:\n",
    "        a modified \"controller.js\" that includes a new project\n",
    "    \"\"\"\n",
    "    # read exisitng controller.js and parse by lines\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'r') as file:\n",
    "        controller = file.read()\n",
    "\n",
    "    controller_parse = controller.split('\\n')\n",
    "\n",
    "    # add idToName after '// add new idToName'\n",
    "    ix = controller_parse.index('\\t\\t\\t\\t\\t// add new idToName')\n",
    "    new_idToName = '\\t\\t\\t\\t\\t\"SHA\":\"SHA\",'.replace('SHA', project_name)\n",
    "    controller_parse.insert(ix+1, new_idToName)\n",
    "\n",
    "    # add selected dataset after '// add new selected dataset here'\n",
    "    ix = controller_parse.index('\\t// add new selected dataset here')\n",
    "    new_selectedDataset = '\\tif (selected_data===\"SHA\") {\\n\\t\\tpopulate_tweets_SHA();\\n\\t\\tpopulate_bins_SHA();\\n\\t\\tpopulate_similarity_SHA();\\n\\t}'.replace('SHA', project_name)\n",
    "    controller_parse.insert(ix+1, new_selectedDataset)\n",
    "\n",
    "    # replace existing controller.js\n",
    "    controller_combine = '\\n'.join(controller_parse)\n",
    "    os.remove(os.path.join(path_tf, 'scripts', 'controller.js'))\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'w') as file:\n",
    "        file.write(controller_combine)\n",
    "\n",
    "    print('controller.js modified,     100% complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"controller.js modified,     100% complete.\" will be printed out in the terminal. The new *controller.js* should have the following changes being made. We still use the example of the new project called \"FD2014\", notice here that the function names (created in function **transform_doc**) will have the project name \"FD2014\" at the end:\n",
    "```javascript\n",
    "var idToName = {\n",
    "\t\t\t\t\t// add new idToName\n",
    "\t\t\t\t\t\"FD2014\":\"FD2014\",\n",
    "\t\t\t\t\t\"Full_Disclosure_2012\":\"Full_Disclosure_2012\"\n",
    "                }\n",
    "```\n",
    "and\n",
    "```javascript\n",
    "// Populate the interface with the selected data set\n",
    "if (selected_data===\"Full_Disclosure_2012\") {\n",
    "    populate_tweets_Full_Disclosure_2012();\n",
    "    populate_bins_Full_Disclosure_2012();\n",
    "    populate_similarity_Full_Disclosure_2012();\n",
    "}\n",
    "// add new selected dataset here\n",
    "if (selected_data===\"FD2014\") {\n",
    "    populate_tweets_FD2014();\n",
    "    populate_bins_FD2014();\n",
    "    populate_similarity_FD2014();\n",
    "}\n",
    "// end of adding new selected datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 6 - del_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the inverse of **modify_html** and **modify_controller**, funtion **del_project** deletes the content related to the specified project(s) in *index.html* and *controller.js*, as well as the project folder under */data*. Human marked comments like \"// Populate the interface with the selected data set\" are used to locate the content. The overall flow looks like: \n",
    "\n",
    "![del_project](https://github.com/estepona/PERCEIVE-freddie/blob/master/notebook_graphs/del_project.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def del_project(project_name_delete):\n",
    "    \"\"\"\n",
    "    Delete an existing project. Content of the project in index.html, \n",
    "    controller.js, and data/<project> folder will be deleted. The base project \n",
    "    \"Full_Disclosure_2012\" should not be deleted.\n",
    "    \n",
    "    Args:\n",
    "        project_name_delete -- name of the project that should be deleted\n",
    "    \n",
    "    Outcome:\n",
    "        Removal of an existing project or multiple existing projects.\n",
    "    \"\"\"\n",
    "    ### DELETE CONTENT IN index.html\n",
    "    # read exisitng index.html and parse by lines\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'r') as file:\n",
    "        html = file.read()\n",
    "    html_parse = html.split('\\n')\n",
    "\n",
    "    # delete section after '<!-- add new section after this line -->'\n",
    "    ix_1 = html_parse.index('<!-- add new section after this line -->')\n",
    "    ix_2 = html_parse.index('<!-- end of adding new datasets. -->')\n",
    "    delete_ix = 0\n",
    "    for i_1 in range(ix_1, ix_2):\n",
    "        # make sure only the specified project is deleted, we don't want to delete other projects that have the this name in it\n",
    "        if project_name_delete in html_parse[i_1] and 'Doc.js' in html_parse[i_1] and len(html_parse[i_1]) == 36+len(project_name_delete):\n",
    "            delete_ix = i_1\n",
    "    for i_2 in range(4): # there are 4 lines for each project section, and we don't want to delete the end line\n",
    "        if not delete_ix == 0:\n",
    "            html_parse.pop(delete_ix)\n",
    "\n",
    "    # delete dataset selector after '<!-- add new dataset selector after this line -->'\n",
    "    ix_1 = html_parse.index('\\t\\t\\t<!-- add new dataset selector after this line -->')\n",
    "    ix_2 = html_parse.index('\\t\\t\\t<!-- end of adding new dataset selector -->')\n",
    "    for i_3 in range(ix_1, ix_2):\n",
    "        if 'id=\"' + project_name_delete + '\"' in html_parse[i_3]:\n",
    "            html_parse.pop(i_3)\n",
    "    \n",
    "    # replace existing index.html\n",
    "    html_combine = '\\n'.join(html_parse)\n",
    "    os.remove(os.path.join(path_tf, 'index.html'))\n",
    "    with open(os.path.join(path_tf, 'index.html'), 'w') as file:\n",
    "        file.write(html_combine)\n",
    "    \n",
    "    \n",
    "    ### DELETE CONTENT IN controller.js\n",
    "    # read exisitng controller.js and parse by lines\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'r') as file:\n",
    "        controller = file.read()\n",
    "    controller_parse = controller.split('\\n')\n",
    "\n",
    "    # delete idToName after '// add new idToName'\n",
    "    ix_1 = controller_parse.index('\\t\\t\\t\\t\\t// add new idToName')\n",
    "    ix_2 = controller_parse.index('\\t\\t\\t\\t\\t\"Full_Disclosure_2012\":\"Full_Disclosure_2012\"')\n",
    "    for i_4 in range(ix_1, ix_2):\n",
    "        if '\"' + project_name_delete + '\"' in controller_parse[i_4]:\n",
    "            controller_parse.pop(i_4)\n",
    "\n",
    "    # delete selected dataset after '// add new selected dataset here'\n",
    "    ix_1 = controller_parse.index('\\t// add new selected dataset here')\n",
    "    ix_2 = controller_parse.index('\\t// end of adding new selected datasets')\n",
    "    delete_ix = 0\n",
    "    for i_5 in range(ix_1, ix_2):\n",
    "        if '\"' + project_name_delete + '\"' in controller_parse[i_5]:\n",
    "            delete_ix = i_5\n",
    "    for i_6 in range(5): # there are 5 lines for each selected dataset, and we don't want to delete the end line\n",
    "        if not delete_ix == 0:\n",
    "            controller_parse.pop(delete_ix)\n",
    "\n",
    "    # replace existing controller.js\n",
    "    controller_combine = '\\n'.join(controller_parse)\n",
    "    os.remove(os.path.join(path_tf, 'scripts', 'controller.js'))\n",
    "    with open(os.path.join(path_tf, 'scripts', 'controller.js'), 'w') as file:\n",
    "        file.write(controller_combine)\n",
    "    \n",
    "    \n",
    "    ### DELETE data.<project_name_delete> FOLDER\n",
    "    # delete three .js files\n",
    "    for js_file in os.listdir(os.path.join(path_tf, 'data', project_name_delete)):\n",
    "        os.remove(os.path.join(path_tf, 'data', project_name_delete, js_file))\n",
    "    # delete project folder\n",
    "    os.rmdir(os.path.join(path_tf, 'data', project_name_delete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the modification, a line says \"Project(s) successfully deleted, running TopicFlow now...\" will be printed out in the terminal. Then, if you examine *index.html*, *controller.js*, or the */data* directory, you will no longer see the specified project(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 7 - argparse and local server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, under   \n",
    ">`if __name__ == \"__main__\":`  \n",
    "\n",
    "Three functionalities are added to allow malnipulation in terminal and invoke local server instance. \n",
    "\n",
    "Using the argparse library in `run.py` makes it easier for a user to add or delete project(s) in command lines and see the TopicFlow visualization in a local server, or simple run an existing project.\n",
    "\n",
    "The function will first check if a \"delete\" option is called. If called, only the deletion part will be executed. Then the function checks if an \"add\" optin is called. If called, a new project will be added. If nothing is called, `python run.py` will invoke a local server instance with a randomly generated port number, which will also be printed out in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # record the path of topicflow\n",
    "    path_tf = sys.argv[0][:-6]\n",
    "    if len(path_tf) == 0:\n",
    "        path_tf = '.'\n",
    "\n",
    "    ### ARGPARSE\n",
    "    parser = argparse.ArgumentParser(prog = 'run.py',\n",
    "                                     description = 'A program that lets you create a new project and transforms your data into TopicFlow readable format, or just run TopicFlow and choose existing projects with the command \"python run.py\".',\n",
    "                                     epilog = 'Then you can open a browser and type in localhost:<port number> to see the visualization! When done, just stop the process in terminal.')\n",
    "    parser.add_argument('-a', '--add', type = str, nargs = '+',\n",
    "                        help = 'If adding a new project. Please specify all the following items: [the name of the project, path of document folder, path of document metadata folder, document extension, path of LDA folder], 5 items in total. Enclosing each in double quotes, and don\\'t forget the dots. Please don\\'t use space when naming. For the document extension, choose from [.reply.body.txt, .reply.body_no_signature.txt, .reply.body_tags.txt, .reply.title_body.txt, .reply.title_body_no_signature.txt]. If running an existing project, no need to use this flag. EXAMPLE: python run.py -a \"FD2014\" \"E:\\\\...\\\\data\\\\docs\" \"E:\\\\...\\\\data\\\\docs_metadata\" \".reply.body.txt\" \"E:\\\\...\\\\data\\\\LDA\".')\n",
    "    parser.add_argument('-d', '--delete', type = str, nargs = '+',\n",
    "                        help = 'Delete one or multiple existing projects. Specify the name(s) of the project(s) that should be deleted in double quotes. The base project \"Full_Disclosure_2012\" should not be deleted. Single deletion example: python run.py -d \"FD2014\". Multiple deletion example: python run.py -d \"FD2014\" \"FD2015\".')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # delete an existing project, if true, end the outer if.\n",
    "    if args.delete:\n",
    "        for arg_del in args.delete:\n",
    "            if os.path.isdir(os.path.join(path_tf, 'data', arg_del)):\n",
    "                project_name_delete = arg_del\n",
    "                del_project(project_name_delete)\n",
    "        if len(args.delete) == 1:\n",
    "            print('Project successfully deleted, running TopicFlow now...')\n",
    "        elif len(args.delete) >= 1:\n",
    "            print('Projects successfully deleted, running TopicFlow now...')\n",
    "    \n",
    "    # add a new project\n",
    "    elif args.add:\n",
    "        project_name = args.add[0]\n",
    "        path_doc = args.add[1]\n",
    "        path_meta = args.add[2]\n",
    "        doc_extension = args.add[3]\n",
    "        path_LDA = args.add[4]\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        if os.path.isdir(path_doc) and os.path.isdir(path_LDA):\n",
    "            print('\\nData transformation started...')\n",
    "            tweet_id_txt = transform_doc(project_name, path_doc, path_meta, doc_extension)\n",
    "            transform_bins(project_name, path_doc, path_meta, path_LDA, tweet_id_txt)\n",
    "            transform_topicSimilarity(project_name, path_LDA)\n",
    "            modify_html(project_name, path_tf)\n",
    "            modify_controller(project_name, path_tf)\n",
    "            print('\\nTotal time taken:', str(round(time.time() - time_start, 2)), 'seconds.\\n')\n",
    "\n",
    "\n",
    "    ### INVOKE SERVER\n",
    "    PORT = np.random.randint(9000, 10000)\n",
    "\n",
    "    # change the working directory to topicflow\n",
    "    os.chdir(path_tf)\n",
    "\n",
    "    Handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "    with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n",
    "        print(\"serving at port\", PORT)\n",
    "        httpd.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a terminal, the above codes generate the following options:\n",
    "\n",
    "command:  \n",
    "`python run.py -h`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example\\_-h](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/example_-h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new project called \"FD2014\" using Full Disclosure 2014 documents and 2014 LDA data in terminal looks like: (note the five arguments)\n",
    "\n",
    "command:  \n",
    "`python run.py -a \"FD2014\" \"E:\\documents\\Learning Materials\\from_UMD\\projects\\PERCEIVE\\data\\New Crawler Full Disclosure\\2014.parsed\" \"E:\\documents\\Learning Materials\\from_UMD\\projects\\PERCEIVE\\data\\New Crawler Full Disclosure\\2014.csv\" \".reply.body_no_signature.txt\" \"E:\\documents\\Learning Materials\\from_UMD\\projects\\PERCEIVE\\data\\LDA_VEM\\estepona_2014_k_10\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example_FD2014](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/example_FD2014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A timer is also included to see how long the data transformation takes. Just nice to know.\n",
    "\n",
    "Now, let us see the end result of our data transformation in browser!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example_FD2014_screenshot](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/example_FD2014_screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also **search** a term in the left-bottom panel. In the example below, a search term \"security\" results in a number of highlightd cells that are related to \"security\".\n",
    "\n",
    "![example_FD2014_screenshot_search](https://raw.githubusercontent.com/estepona/PERCEIVE-freddie/master/notebook_graphs/example_FD2014_screenshot_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with the visualization, just stop the process in terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although now we have a working data transformation pipeline, there are still one issue remained:\n",
    "\n",
    "    \n",
    "**Value of node**  \n",
    "    In function **transform_topicSimilarity**, the value of each individual node is not clearly defined in the original paper, so the way I approach this is generating a random integer between 1 and 100 and assign it to the value of node.\n",
    "```python\n",
    "# how to calculate the value of a topic? the paper didn't define clearly\n",
    "# so here I use a random number\n",
    "value = np.random.randint(1,100)\n",
    "tmp['name'], tmp['value'] = name, value\n",
    "```\n",
    "We'd like to know how the values are defined and make changes to the data transformation pipeline accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
